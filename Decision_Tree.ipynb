{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfo0oX1ZDe1wNYoVK4CbFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seyjuti8884/pwskills_assignment/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi-rbmbQN_ZL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions\n",
        "\n",
        "1. **What is a Decision Tree, and how does it work?**  \n",
        "   A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It splits data based on feature values using a tree-like structure where each internal node represents a decision based on an attribute, each branch represents an outcome of that decision, and each leaf node represents a class label or a continuous value (in regression).\n",
        "\n",
        "2. **What are impurity measures in Decision Trees?**  \n",
        "   Impurity measures quantify how mixed a dataset is at a particular node. Common impurity measures include **Gini Impurity**, **Entropy**, and **Misclassification Rate**.\n",
        "\n",
        "3. **What is the mathematical formula for Gini Impurity?**  \n",
        "   \\[\n",
        "   Gini = 1 - Σ (pᵢ²)\n",
        "   \\]\n",
        "   where \\( p_i \\) is the probability of a particular class at a node.\n",
        "\n",
        "4. **What is the mathematical formula for Entropy?**  \n",
        "   \\[\n",
        "   Entropy = - Σ (pᵢ * log₂ pᵢ)\n",
        "   \\]\n",
        "   where \\( p_i \\) is the probability of a class at a node.\n",
        "\n",
        "5. **What is Information Gain, and how is it used in Decision Trees?**  \n",
        "   Information Gain measures the reduction in impurity when splitting a node. It is calculated as the difference between the impurity of the parent node and the weighted impurity of the child nodes. Higher Information Gain means a better split.\n",
        "\n",
        "6. **What is the difference between Gini Impurity and Entropy?**  \n",
        "   - **Gini Impurity** is computationally faster and measures the probability of misclassification.  \n",
        "   - **Entropy** measures the amount of information disorder in a dataset.  \n",
        "\n",
        "7. **What is the mathematical explanation behind Decision Trees?**  \n",
        "   Decision Trees use recursive binary splitting by selecting the feature that maximizes **Information Gain** (for Entropy) or minimizes **Gini Impurity**. The tree grows by applying this process to subsets of data until a stopping criterion (like minimum samples per leaf or maximum depth) is met.\n",
        "\n",
        "8. **What is Pre-Pruning in Decision Trees?**  \n",
        "   Pre-pruning stops tree growth early using constraints such as **maximum depth, minimum samples per split, or minimum impurity decrease** to avoid overfitting.\n",
        "\n",
        "9. **What is Post-Pruning in Decision Trees?**  \n",
        "   Post-pruning involves growing the tree fully and then pruning back branches that do not improve accuracy using techniques like **cost-complexity pruning (CCP)**.\n",
        "\n",
        "10. **What is the difference between Pre-Pruning and Post-Pruning?**  \n",
        "    - **Pre-Pruning** stops tree growth early based on predefined constraints.  \n",
        "    - **Post-Pruning** first allows full tree growth and then removes unnecessary branches.\n",
        "\n",
        "11. **What is a Decision Tree Regressor?**  \n",
        "    A Decision Tree Regressor is a Decision Tree model used for regression tasks. Instead of predicting class labels, it predicts continuous values by minimizing the Mean Squared Error (MSE) in splits.\n",
        "\n",
        "12. **What are the advantages and disadvantages of Decision Trees?**  \n",
        "    **Advantages:**  \n",
        "    - Simple and interpretable  \n",
        "    - Requires minimal data preprocessing  \n",
        "    - Works with both numerical and categorical data  \n",
        "    - Handles non-linearity well  \n",
        "\n",
        "    **Disadvantages:**  \n",
        "    - Prone to overfitting  \n",
        "    - Sensitive to small data variations  \n",
        "    - Greedy splitting may lead to suboptimal solutions  \n",
        "\n",
        "13. **How does a Decision Tree handle missing values?**  \n",
        "    - It can ignore missing values and proceed with available data.  \n",
        "    - It can use surrogate splits (alternative splits for missing values).  \n",
        "    - It can replace missing values with the most frequent or mean value.\n",
        "\n",
        "14. **How does a Decision Tree handle categorical features?**  \n",
        "    - It can split on categorical values directly (for small categories).  \n",
        "    - It can use one-hot encoding or ordinal encoding.  \n",
        "\n",
        "15. **What are some real-world applications of Decision Trees?**  \n",
        "    - **Medical diagnosis** (predicting disease based on symptoms).  \n",
        "    - **Finance** (credit risk assessment).  \n",
        "    - **Customer segmentation** (identifying target groups).  \n",
        "    - **Fraud detection** (detecting fraudulent transactions).  \n",
        "    - **Recommendation systems** (suggesting products based on user behavior).  \n",
        "\n"
      ],
      "metadata": {
        "id": "GXKwElCHP5uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions"
      ],
      "metadata": {
        "id": "bd6bnbCYQoY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **16. Train a Decision Tree Classifier on the Iris dataset and print the model accuracy**\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Model Accuracy: 1.0 (or close to 0.95-1.0)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **17. Train a Decision Tree Classifier using Gini Impurity and print feature importances**\n",
        "```python\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Feature Importances: [0.02, 0.01, 0.57, 0.40]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **18. Train a Decision Tree Classifier using Entropy and print model accuracy**\n",
        "```python\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Model Accuracy: 1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **19. Train a Decision Tree Regressor on a housing dataset and evaluate using MSE**\n",
        "```python\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Mean Squared Error: ~0.5-1.5 (varies based on train-test split)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **20. Train a Decision Tree Classifier and visualize the tree using Graphviz**\n",
        "```python\n",
        "from sklearn.tree import export_text, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Export tree structure\n",
        "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "\n",
        "# Visualize\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves the tree as a file\n",
        "graph.view()\n",
        "```\n",
        "**Expected Output:**  \n",
        "A visual tree structure saved as `decision_tree.pdf` or displayed.\n",
        "\n",
        "---\n",
        "\n",
        "### **21. Train a Decision Tree Classifier with max depth = 3 and compare accuracy**\n",
        "```python\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "accuracy_full = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_depth3)\n",
        "print(\"Accuracy with fully grown tree:\", accuracy_full)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Accuracy with max_depth=3: ~0.90-0.95\n",
        "Accuracy with fully grown tree: ~1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **22. Train a Decision Tree Classifier with `min_samples_split=5` and compare accuracy**\n",
        "```python\n",
        "clf_split5 = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_split5.fit(X_train, y_train)\n",
        "y_pred_split5 = clf_split5.predict(X_test)\n",
        "\n",
        "accuracy_split5 = accuracy_score(y_test, y_pred_split5)\n",
        "accuracy_default = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "print(\"Accuracy with min_samples_split=5:\", accuracy_split5)\n",
        "print(\"Accuracy with default tree:\", accuracy_default)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Accuracy with min_samples_split=5: ~0.95-1.0\n",
        "Accuracy with default tree: 1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **23. Apply feature scaling before training a Decision Tree and compare accuracy**\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy with feature scaling:\", accuracy_scaled)\n",
        "print(\"Accuracy without scaling:\", accuracy)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Accuracy with feature scaling: Similar to unscaled (Decision Trees are insensitive to scaling)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **24. Train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification**\n",
        "```python\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ovr = ovr_clf.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "print(\"OvR Decision Tree Accuracy:\", accuracy_ovr)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "OvR Decision Tree Accuracy: ~0.95-1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **25. Train a Decision Tree Classifier and display feature importance scores**\n",
        "```python\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Feature Importances: [0.02, 0.03, 0.55, 0.40]  # Values may vary\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **26. Train a Decision Tree Regressor with max_depth=5 and compare performance**\n",
        "```python\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "print(\"MSE with max_depth=5:\", mse_depth5)\n",
        "print(\"MSE with unrestricted tree:\", mse)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "MSE with max_depth=5: ~0.8-1.2\n",
        "MSE with unrestricted tree: ~0.5-1.5\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **27. Train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize effect on accuracy**\n",
        "```python\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    print(f\"Alpha: {alpha}, Accuracy: {accuracy_score(y_test, clf_pruned.predict(X_test))}\")\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Alpha: 0.0, Accuracy: 1.0\n",
        "Alpha: 0.01, Accuracy: ~0.95\n",
        "Alpha: 0.1, Accuracy: ~0.90\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **28. Train a Decision Tree Classifier and evaluate performance using Precision, Recall, and F1-Score**\n",
        "```python\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Precision: ~1.0\n",
        "Recall: ~1.0\n",
        "F1-Score: ~1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **29. Train a Decision Tree Classifier and visualize the confusion matrix using Seaborn**\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "```\n",
        "**Expected Output:**  \n",
        "A heatmap showing the confusion matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### **30. Train a Decision Tree Classifier and use GridSearchCV to find optimal `max_depth` and `min_samples_split`**\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Best Parameters: {'max_depth': 5, 'min_samples_split': 2}  # Varies based on data\n",
        "Best Accuracy: ~0.95-1.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FzCmxh9TVAwY"
      }
    }
  ]
}