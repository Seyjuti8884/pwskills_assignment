{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPHecOlhPR2MQy4q6SptZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seyjuti8884/pwskills_assignment/blob/main/Boosting_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NZ6448dypuI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions\n",
        "\n",
        "1. **What is Boosting in Machine Learning?**  \n",
        "   Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a strong learner. It sequentially trains models so that each new model corrects the errors of the previous ones, improving overall prediction accuracy.\n",
        "\n",
        "2. **How does Boosting differ from Bagging?**  \n",
        "   - **Boosting:** Models are trained sequentially, with each new model focusing on the errors of the previous one. It reduces bias.  \n",
        "   - **Bagging:** Models are trained independently and in parallel on different subsets of the data, reducing variance (e.g., Random Forest).\n",
        "\n",
        "3. **What is the key idea behind AdaBoost?**  \n",
        "   AdaBoost (Adaptive Boosting) assigns higher weights to misclassified samples and lower weights to correctly classified ones in each iteration. The weak learners are combined using weighted majority voting to create a strong learner.\n",
        "\n",
        "4. **Explain the working of AdaBoost with an example.**  \n",
        "   - Start with equal weights for all samples.  \n",
        "   - Train a weak learner (e.g., a decision stump).  \n",
        "   - Misclassified samples get higher weights.  \n",
        "   - Train the next weak learner with updated weights.  \n",
        "   - Continue this process, combining weak learners to form a strong model.  \n",
        "   Example: Classifying emails as spam/non-spam using weighted decision trees.\n",
        "\n",
        "5. **What is Gradient Boosting, and how is it different from AdaBoost?**  \n",
        "   - **Gradient Boosting** minimizes a loss function by fitting new weak learners to the residual errors of the previous model, using gradient descent.  \n",
        "   - **Difference:** AdaBoost adjusts sample weights, whereas Gradient Boosting optimizes the loss function using gradients.\n",
        "\n",
        "6. **What is the loss function in Gradient Boosting?**  \n",
        "   The loss function in Gradient Boosting depends on the task:  \n",
        "   - **Regression:** Mean Squared Error (MSE) or Mean Absolute Error (MAE).  \n",
        "   - **Classification:** Log Loss or Cross-Entropy Loss.\n",
        "\n",
        "7. **How does XGBoost improve over traditional Gradient Boosting?**  \n",
        "   XGBoost (Extreme Gradient Boosting) offers:  \n",
        "   - Regularization to prevent overfitting.  \n",
        "   - Parallelization for faster training.  \n",
        "   - Handling of missing values.  \n",
        "   - Tree pruning for efficiency.\n",
        "\n",
        "8. **What is the difference between XGBoost and CatBoost?**  \n",
        "   - **XGBoost:** Optimized for speed, supports missing values, and uses greedy tree pruning.  \n",
        "   - **CatBoost:** Specialized for categorical data, using ordered boosting to prevent target leakage.\n",
        "\n",
        "9. **What are some real-world applications of Boosting techniques?**  \n",
        "   - Fraud detection (e.g., banking).  \n",
        "   - Customer churn prediction.  \n",
        "   - Medical diagnosis.  \n",
        "   - Spam detection.  \n",
        "   - Recommendation systems.\n",
        "\n",
        "10. **How does regularization help in XGBoost?**  \n",
        "    Regularization (L1 & L2) helps by:  \n",
        "    - Preventing overfitting.  \n",
        "    - Reducing model complexity.  \n",
        "    - Improving generalization.\n",
        "\n",
        "11. **What are some hyperparameters to tune in Gradient Boosting models?**  \n",
        "    - Learning rate.  \n",
        "    - Number of estimators.  \n",
        "    - Maximum depth of trees.  \n",
        "    - Subsample ratio.  \n",
        "    - Minimum child weight.\n",
        "\n",
        "12. **What is the concept of Feature Importance in Boosting?**  \n",
        "    Feature importance ranks the most influential variables in model predictions. It helps in:  \n",
        "    - Feature selection.  \n",
        "    - Understanding model decisions.  \n",
        "    - Reducing dimensionality.\n",
        "\n",
        "13. **Why is CatBoost efficient for categorical data?**  \n",
        "    - Uses ordered boosting to prevent data leakage.  \n",
        "    - Handles categorical variables without needing one-hot encoding.  \n",
        "    - Uses efficient GPU support for fast training.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhUUL2yByq0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions\n",
        "\n",
        "## 14. **Train an AdaBoost Classifier on a sample dataset and print model accuracy**\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and print accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "**Expected Output (Varies slightly on each run):**\n",
        "```\n",
        "AdaBoost Classifier Accuracy: ~0.85 - 0.90\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 15. **Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)**\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "regressor = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MAE\n",
        "y_pred = regressor.predict(X_test)\n",
        "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Mean Absolute Error (MAE): ~5.0 - 10.0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 16. **Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance**\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gbc.fit(X, y)\n",
        "\n",
        "# Print feature importance\n",
        "feature_importance = pd.DataFrame({'Feature': data.feature_names, 'Importance': gbc.feature_importances_})\n",
        "print(feature_importance.sort_values(by=\"Importance\", ascending=False))\n",
        "```\n",
        "**Expected Output:**  \n",
        "A table showing the most important features, e.g.:\n",
        "```\n",
        "        Feature         Importance\n",
        "0   worst radius       0.20\n",
        "1   mean texture      0.12\n",
        "2   worst perimeter   0.10\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 17. **Train a Gradient Boosting Regressor and evaluate using R-Squared Score**\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate R^2 score\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(\"R-Squared Score:\", r2_score(y_test, y_pred))\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "R-Squared Score: ~0.85 - 0.95\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 18. **Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting**\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compare accuracy\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "XGBoost Classifier Accuracy: ~0.87 - 0.92\n",
        "```\n",
        "(This will be compared with the Gradient Boosting accuracy)\n",
        "\n",
        "---\n",
        "\n",
        "## 19. **Train a CatBoost Classifier and evaluate using F1-Score**\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cbc = CatBoostClassifier(iterations=100, verbose=0)\n",
        "cbc.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute F1-score\n",
        "y_pred_cbc = cbc.predict(X_test)\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred_cbc))\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "F1-Score: ~0.85 - 0.90\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 20. **Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)**\n",
        "```python\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgb_reg = XGBRegressor(n_estimators=100)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate MSE\n",
        "y_pred_xgb_reg = xgb_reg.predict(X_test)\n",
        "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred_xgb_reg))\n",
        "```\n",
        "**Expected Output:**\n",
        "```\n",
        "Mean Squared Error (MSE): ~10.0 - 20.0\n",
        "```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "-Z1ixAXlzD2x"
      }
    }
  ]
}